# Enterprise RAG System (Retrieval-Augmented Generation)

## ğŸ“Œ Overview

This project implements an end-to-end Retrieval-Augmented Generation (RAG) pipeline that enhances LLM responses by retrieving relevant contextual information before generating answers.

RAG improves factual grounding by combining:

1. Dense vector retrieval
2. Semantic search
3. Large Language Model generation

---

## ğŸ—ï¸ Architecture

User Query
   â†“
Query Embedding (Sentence Transformer)
   â†“
FAISS Vector Search
   â†“
Top-K Relevant Context Retrieval
   â†“
LLM Prompt Construction
   â†“
Generated Answer

---

## ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) is a hybrid architecture that:

- Retrieves relevant information from a knowledge base
- Injects retrieved context into a prompt
- Uses an LLM to generate grounded responses

This reduces hallucination and improves response accuracy.

---

## ğŸ” Embedding Model Used

Model: `all-MiniLM-L6-v2`  
Library: Sentence Transformers  

This model converts text into dense vector representations for semantic similarity search.

---

## ğŸ“¦ Vector Database

FAISS (Facebook AI Similarity Search)

Index Type: `IndexFlatL2`

- Uses L2 (Euclidean) distance
- Suitable for smaller datasets
- Performs exact nearest neighbor search

---

## ğŸ¤– LLM Used

Model: GPT-2  
Library: HuggingFace Transformers  

Used for text generation based on retrieved context.

---

## âš™ï¸ Technologies

- Python
- Sentence Transformers
- FAISS
- HuggingFace Transformers
- Modular architecture

---

## ğŸš€ How to Run

```bash
pip install -r requirements.txt
python src/main.py
